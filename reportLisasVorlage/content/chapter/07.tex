%!TEX root = ../../main.tex

\chapter{Conclusion}
\label{chap:conclusion}
We reached all main goals for the project. The best model has a decent accuracy and we have learned many things about machine learning. Through the team work with SCRUM we were able to delegate the tasks in a way, that we get the best end solution. Additionally we learned how to apply SCRUM for team work, which will help us in our further working life. %All steps were necessary for the final outcome, this means we did not got stuck in a wrong direction. The project is proper documented, that a future team is able to continue with our work on the project. The biggest issues we had especially in the beginning of the project, because we did not really know how we should reach our goals or what goals we really had. For the future it would be awesome if the communication between the product owners and the students would be better. Not only in how we should start and what are the main goals, but also how the grade will be exactly built and how we have to structure the report. It would be a good way to create a one- or two-paper with all the common guidelines that the students know exactly what the product owners are require and how they will evaluate the outcome. We are not in a stage that we could actually really earn very much money with our model, which means, that we are only hardly over a 50\% accuracy. We are still have the goal to improve our model to reach higher accuracies. For the next semester the main part will be improving the model through additional features and through changing the model.
\\ \\
From the european soccer database, we extracted several different features, like amount of goals shot by each team, amount of wins, draws and losses for each team during the last 10 matches, shot-accuracy and shot-efficiency per team and the overall ball possession of each soccer-team per match.\newline
Using an algorithm which we adapted for our special data \cite{sliding01} we created a sliding window which aggregates the extracted features over the last 10 soccer-games for each match in the database. Because a lot of data samples in the database were incomplete regarding some of the extracted features, we created 5 different versions of the sliding window. Each containing a different amount of features and therefore a different amount of data samples. This gives us the possibility to test classifiers with various different models and later pick the solution that works best. The sliding window option 1 uses the least amount of features (13 features) and has the highest number of data samples (20823 data samples). Option 2 has 21 features and 7033 data samples, option 3 uses 29 features and 7033 samples, option 4 contains 25 features and 6996 samples and option 5 has the most features (33) and 6996 data samples.\\
\\
We used the 5 sliding window options to train and test various classifiers such as a Decision Tree, a Multi-Layer Perceptron and various different basic sequential neural nets. For the Decision Tree and the Multi-Layer Perceptron we used the scikit-learn API. For the basic sequential neural nets we used the Tensorflow/Keras framework, which is state of the art when doing data analytics tasks.\newline
The decision tree using the first sliding window option and a depth of 4 gives us a test-accuracy of 52,95\%. The Multi-Layer Perceptron uses the sliding window option 1 aswell and has a test-accuracy of 53,45\%. It has 2 hidden layers with 52 and 32 neurons. The best version of the Keras Sequential Neural Network was trained with sliding window option 3, has 2 hidden layers (21, 21 neurons) and a test-accuracy of 56,25\%. It is also the best classifier in general we were able to find until now. At first sight model accuracies barely over 50\% do not seem very good, but considering that the home team has a base chance of 46\% to win the match and soccer is still a gambling game up to some point those accuracies are not too bad at all.\\
\\
Nevertheless, the team will continue its work on the project striving for better models by using different/additional features, adapting the split of training- and test-data, trying other kinds of classifiers and tweaking the existing models by changing some parameters and input functions.